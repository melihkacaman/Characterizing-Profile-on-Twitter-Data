{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b979f8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78fbf171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk                                # a Python NLP library  \n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random                              # pseudo-random number generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaefd11",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7846f225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1515056253793447944</td>\n",
       "      <td>Scott Pruitt, Trump's former EPA chief,  is ru...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1515040871418261506</td>\n",
       "      <td>RT @politicongress: A joint fundraising commit...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1515017655182409731</td>\n",
       "      <td>From their earliest days, Black churches have ...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1515011952237588485</td>\n",
       "      <td>RT @playbookdc: In early November, Rep. Chip R...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1515002027079872528</td>\n",
       "      <td>Add West Wing Playbook to your daily reads for...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             tweet_id  \\\n",
       "0           0  1515056253793447944   \n",
       "1           1  1515040871418261506   \n",
       "2           2  1515017655182409731   \n",
       "3           3  1515011952237588485   \n",
       "4           4  1515002027079872528   \n",
       "\n",
       "                                                text    source  \n",
       "0  Scott Pruitt, Trump's former EPA chief,  is ru...  politico  \n",
       "1  RT @politicongress: A joint fundraising commit...  politico  \n",
       "2  From their earliest days, Black churches have ...  politico  \n",
       "3  RT @playbookdc: In early November, Rep. Chip R...  politico  \n",
       "4  Add West Wing Playbook to your daily reads for...  politico  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food = pd.read_csv('food_usa.csv')\n",
    "data_politics = pd.read_csv('politics_usa.csv')\n",
    "data_magazine = pd.read_csv('magazine_usa.csv')\n",
    "data_sport = pd.read_csv('sport_usa.csv')\n",
    "data_technology = pd.read_csv('technology_usa.csv')\n",
    "\n",
    "data_politics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e988471",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fitness = pd.read_csv('fitnes-health_usa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b29812",
   "metadata": {},
   "source": [
    "## Looking at raw-tweets \n",
    "\n",
    "Before anything else, we can print a couple of tweets from our datasets to see how they look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ee7646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLITCS: Macron warns of \"escalation of rhetoric\" after Biden \"genocide\" comment https://t.co/in2JitJDxt https://t.co/kKWZumU8I2\n"
     ]
    }
   ],
   "source": [
    "print(\"POLITCS:\", data_politics.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b710ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOOD: ‚ÄúBurnt‚Äù seems poised to become one more tool brands can use to make it always seem like they‚Äôre coming up with something new https://t.co/n31jyb6fZn\n"
     ]
    }
   ],
   "source": [
    "print(\"FOOD:\", data_food.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ebe2a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAGAZINE: BREAKING: Will Smith has responded to the Academy of Motion Pictures Arts and Sciences‚Äô decision to ban him from all Oscars events for 10 years. https://t.co/Al35k9PR77\n"
     ]
    }
   ],
   "source": [
    "print(\"MAGAZINE:\", data_magazine.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d634be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECHNOLOGY: Apple turns monitor height adjustment into a $400 upsell https://t.co/kPoMJv5MOp https://t.co/m9YGAPvu4j\n"
     ]
    }
   ],
   "source": [
    "print(\"TECHNOLOGY:\", data_technology.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "961cee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPORT: RT @FOXBetLive: World Series Picks ‚öæÔ∏è üèÜ\n",
      "\n",
      "@spshoot: Dodgers (+500)\n",
      "@BenVerlander: Blue Jays (+900)\n",
      "\n",
      "Which bet are you taking? https://t.co/f‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "print(\"SPORT:\", data_sport.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e9124",
   "metadata": {},
   "source": [
    "# Preprocess raw text for Sentiment analysis\n",
    "\n",
    "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "\n",
    "- Tokenizing the string\n",
    "- Lowercasing\n",
    "- Removing stop words and punctuation\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4539f2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boost your hair care routine! #Scream star #JennaOrtega revealed her go-to beauty regimen for her luscious locks. Shop her favorite products now!\\nhttps://t.co/uwvRvZFI94'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = data_magazine.loc[5].text \n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21466c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\melih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b75472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081089d",
   "metadata": {},
   "source": [
    "# Remove hyperlinks, Twitter marks and styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87e29bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boost your hair care routine! Scream star JennaOrtega revealed her go-to beauty regimen for her luscious locks. Shop her favorite products now!\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove old style retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "# remove hyperlinks\n",
    "tweet2 = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet2)\n",
    "\n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "tweet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2faddb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boost your hair care routine! #Scream star #JennaOrtega revealed her go-to beauty regimen for her luscious locks. Shop her favorite products now!\\nhttps://t.co/uwvRvZFI94'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5ecdf",
   "metadata": {},
   "source": [
    "# Tokenize the string\n",
    "To tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f987b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c51ab88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boost your hair care routine! Scream star JennaOrtega revealed her go-to beauty regimen for her luscious locks. Shop her favorite products now!\n",
      "\n",
      "['boost', 'your', 'hair', 'care', 'routine', '!', 'scream', 'star', 'jennaortega', 'revealed', 'her', 'go-to', 'beauty', 'regimen', 'for', 'her', 'luscious', 'locks', '.', 'shop', 'her', 'favorite', 'products', 'now', '!']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "print(tweet2)\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59688621",
   "metadata": {},
   "source": [
    "### Remove Stop Words and Punctuations\n",
    "\n",
    "The next step is to remove stop words and punctuation. Stop words are words that don't add significant meaning to the text. \n",
    "You'll see them below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afefffae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuation\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "print('Stop words\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcfe5bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed stop words and punctuation:\n",
      "['boost', 'hair', 'care', 'routine', 'scream', 'star', 'jennaortega', 'revealed', 'go-to', 'beauty', 'regimen', 'luscious', 'locks', 'shop', 'favorite', 'products']\n"
     ]
    }
   ],
   "source": [
    "tweets_clean = []\n",
    "\n",
    "for word in tweet_tokens: \n",
    "    if (word not in stopwords_english and word not in string.punctuation): \n",
    "        tweets_clean.append(word)\n",
    "\n",
    "print('removed stop words and punctuation:')\n",
    "print(tweets_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836cc9d3",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n",
    "\n",
    "Consider the words: \n",
    " * **learn**\n",
    " * **learn**ing\n",
    " * **learn**ed\n",
    " * **learn**t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa263ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75c2cd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed words:\n",
      "['boost', 'hair', 'care', 'routin', 'scream', 'star', 'jennaortega', 'reveal', 'go-to', 'beauti', 'regimen', 'lusciou', 'lock', 'shop', 'favorit', 'product']\n"
     ]
    }
   ],
   "source": [
    "tweets_stem = [] \n",
    "\n",
    "for word in tweets_clean:\n",
    "    stem_word = stemmer.stem(word)  # stemming word\n",
    "    tweets_stem.append(stem_word)  # append to the list\n",
    "\n",
    "print('stemmed words:')\n",
    "print(tweets_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67fbc0",
   "metadata": {},
   "source": [
    "# Create A Function That do pre-processing tweets one by one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24db1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4e3abcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boost',\n",
       " 'hair',\n",
       " 'care',\n",
       " 'routin',\n",
       " 'scream',\n",
       " 'star',\n",
       " 'jennaortega',\n",
       " 'reveal',\n",
       " 'go-to',\n",
       " 'beauti',\n",
       " 'regimen',\n",
       " 'lusciou',\n",
       " 'lock',\n",
       " 'shop',\n",
       " 'favorit',\n",
       " 'product']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d15141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muffin', 'tin', 'great', 'meal', 'prep', 'individu', 'serv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet(data_food.loc[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942bbf5",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27f14946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1516326752834002944</td>\n",
       "      <td>A reminder that delivery app restaurants aren'...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1516318179894640641</td>\n",
       "      <td>Braised chicken legs, soy sauce, vinegar, blac...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1516311648889417728</td>\n",
       "      <td>Muffin tins are great for meal prepping indivi...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1516296543791456260</td>\n",
       "      <td>We tested everything from spicy dill pickle fl...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1516290031522127876</td>\n",
       "      <td>Is eating a regular Big Mac the secret to a lo...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             tweet_id  \\\n",
       "0           0  1516326752834002944   \n",
       "1           1  1516318179894640641   \n",
       "2           2  1516311648889417728   \n",
       "3           3  1516296543791456260   \n",
       "4           4  1516290031522127876   \n",
       "\n",
       "                                                text       source  \n",
       "0  A reminder that delivery app restaurants aren'...  foodandwine  \n",
       "1  Braised chicken legs, soy sauce, vinegar, blac...  foodandwine  \n",
       "2  Muffin tins are great for meal prepping indivi...  foodandwine  \n",
       "3  We tested everything from spicy dill pickle fl...  foodandwine  \n",
       "4  Is eating a regular Big Mac the secret to a lo...  foodandwine  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f12bf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_food_processed = np.array([process_tweet(item) for item in data_food['text'].tolist()], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4809ab38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4881,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food_processed.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01dcf696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['remind', 'deliveri', 'app', 'restaur', 'alway', 'seem']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e994002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muffin', 'tin', 'great', 'meal', 'prep', 'individu', 'serv']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food_processed[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e2f2994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['four', 'ingredi', 'one', 'bowl', 'five', 'minut', 'stir', 'togeth']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food_processed[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18529632",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_magazine_processed = np.array([process_tweet(item) for item in data_magazine['text'].tolist()], dtype=object)\n",
    "data_technolohy_processed = np.array([process_tweet(item) for item in data_technology['text'].tolist()], dtype=object)\n",
    "data_politics_processed = np.array([process_tweet(item) for item in data_politics['text'].tolist()], dtype=object)\n",
    "data_sport_processed = np.array([process_tweet(item) for item in data_sport['text'].tolist()], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1b6c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fitness_processed = np.array([process_tweet(item) for item in data_fitness['text'].tolist()], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68dec468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['internet',\n",
       " 'troll',\n",
       " 'field',\n",
       " 'day',\n",
       " \"jadensmith'\",\n",
       " 'expens',\n",
       " \"willsmith'\",\n",
       " 'son',\n",
       " 'butt',\n",
       " 'joke',\n",
       " 'say',\n",
       " 'like',\n",
       " 'talk',\n",
       " 'topic']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_magazine_processed[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "950135b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earli',\n",
       " 'novemb',\n",
       " 'rep',\n",
       " 'chip',\n",
       " 'roy',\n",
       " 'text',\n",
       " 'mark',\n",
       " 'meadow',\n",
       " 'say',\n",
       " '‚Äú',\n",
       " 'need',\n",
       " 'ammo',\n",
       " '‚Äù',\n",
       " 'former',\n",
       " 'presid',\n",
       " 'donald',\n",
       " \"trump'\",\n",
       " 'effort',\n",
       " '‚Ä¶']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_politics_processed[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9133856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'get',\n",
       " 'rockin',\n",
       " 'foam',\n",
       " 'rollin',\n",
       " 'last',\n",
       " 'time',\n",
       " 'show',\n",
       " 'way',\n",
       " 'use',\n",
       " 'foam',\n",
       " 'roller',\n",
       " 'massag',\n",
       " 'sore',\n",
       " 'muscl',\n",
       " 'use',\n",
       " 'versatil',\n",
       " 'tool',\n",
       " 'stretch',\n",
       " 'far',\n",
       " 'beyond',\n",
       " 'whether',\n",
       " 'pre',\n",
       " 'post-workout',\n",
       " 'stretch',\n",
       " 'time',\n",
       " 'feel',\n",
       " 'differ',\n",
       " 'foam',\n",
       " 'make']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fitness_processed[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d62fc1",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1360c69",
   "metadata": {},
   "source": [
    "![alt text](data.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9ab8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterative = {'fitness' : data_fitness_processed, \n",
    "                  'food' : data_food_processed, \n",
    "                  'magazine' : data_magazine_processed, \n",
    "                  'politics' : data_politics_processed, \n",
    "                  'sport' : data_sport_processed, \n",
    "                  'technology' : data_technolohy_processed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46271638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_iterative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b7b0513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- fitness -----------------------\n",
      "actual: fitness 4665\n",
      "rest  food 5665\n",
      "rest  magazine 6665\n",
      "rest  politics 7665\n",
      "rest  sport 8665\n",
      "rest  technology 9665\n",
      "----------------- food -----------------------\n",
      "actual: food 4881\n",
      "rest  fitness 5881\n",
      "rest  magazine 6881\n",
      "rest  politics 7881\n",
      "rest  sport 8881\n",
      "rest  technology 9881\n",
      "----------------- magazine -----------------------\n",
      "actual: magazine 4954\n",
      "rest  fitness 5954\n",
      "rest  food 6954\n",
      "rest  politics 7954\n",
      "rest  sport 8954\n",
      "rest  technology 9954\n",
      "----------------- politics -----------------------\n",
      "actual: politics 4968\n",
      "rest  fitness 5968\n",
      "rest  food 6968\n",
      "rest  magazine 7968\n",
      "rest  sport 8968\n",
      "rest  technology 9968\n",
      "----------------- sport -----------------------\n",
      "actual: sport 4829\n",
      "rest  fitness 5829\n",
      "rest  food 6829\n",
      "rest  magazine 7829\n",
      "rest  politics 8829\n",
      "rest  technology 9829\n",
      "----------------- technology -----------------------\n",
      "actual: technology 4976\n",
      "rest  fitness 5976\n",
      "rest  food 6976\n",
      "rest  magazine 7976\n",
      "rest  politics 8976\n",
      "rest  sport 9976\n"
     ]
    }
   ],
   "source": [
    "all_data = {}\n",
    "for key,item in data_iterative.items(): \n",
    "    print(\"-----------------\", key ,\"-----------------------\")\n",
    "    actual = item \n",
    "    print(\"actual:\", key, len(item))\n",
    "    \n",
    "    for key_in, item_in in data_iterative.items(): \n",
    "        if key_in == key: \n",
    "            continue \n",
    "        \n",
    "        actual = np.concatenate((actual, item_in[:1000]))\n",
    "        print(\"rest \", key_in, len(actual))\n",
    "    \n",
    "    all_data[key] = {'set' : actual, 'actual_size' : len(item)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3a91b927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'set': array([list(['know', 'effect', 'exercis', 'leg', 'tricki', 'design', 'quick', 'effect', '15', 'min', 'workout', 'knock', 'park', 'next', 'leg', 'day', 'one', 'els', 'back', 'shoulder', 'leg', 'like', 'us', 'üíØ']),\n",
       "        list(['stress', 'üò©', 'tire', 'üò¥', 'worn', 'üò∞', 'common', \"they'r\", 'perfect', 'reason', 'come', 'gym', 'work', 'us', 'promis', 'leav', 'feel', 'littl', 'relax', 'energ', 'readi', 'tackl', \"what'\", 'ahead', \"that'\", 'realaf']),\n",
       "        list(['time', 'get', 'rockin', 'foam', 'rollin', 'last', 'time', 'show', 'way', 'use', 'foam', 'roller', 'massag', 'sore', 'muscl', 'use', 'versatil', 'tool', 'stretch', 'far', 'beyond', 'whether', 'pre', 'post-workout', 'stretch', 'time', 'feel', 'differ', 'foam', 'make']),\n",
       "        ...,\n",
       "        list(['meet', 'grow', 'founder', 'network', 'small', 'group', 'discuss', 'techcrunch', 'earli', 'stage']),\n",
       "        list(['microsoft', 'want', 'build', 'next', 'game', 'cloud']),\n",
       "        list(['onepointon', 'want', 'piec', 'vertic', 'farm', 'strawberri', 'market'])],\n",
       "       dtype=object),\n",
       " 'actual_size': 4665}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['fitness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "17dcfcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in all_data.items(): \n",
    "    pd.DataFrame(data=item['set'], columns=[key]).to_csv(key+f\"_[actual_size={item['actual_size']}]_processed_merge.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18060ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
