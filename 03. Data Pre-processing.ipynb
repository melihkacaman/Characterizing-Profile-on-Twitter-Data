{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23fd1ad5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02393efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk                                # a Python NLP library  \n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random                              # pseudo-random number generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ace13",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae211ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1515056253793447944</td>\n",
       "      <td>Scott Pruitt, Trump's former EPA chief,  is ru...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1515040871418261506</td>\n",
       "      <td>RT @politicongress: A joint fundraising commit...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1515017655182409731</td>\n",
       "      <td>From their earliest days, Black churches have ...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1515011952237588485</td>\n",
       "      <td>RT @playbookdc: In early November, Rep. Chip R...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1515002027079872528</td>\n",
       "      <td>Add West Wing Playbook to your daily reads for...</td>\n",
       "      <td>politico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             tweet_id  \\\n",
       "0           0  1515056253793447944   \n",
       "1           1  1515040871418261506   \n",
       "2           2  1515017655182409731   \n",
       "3           3  1515011952237588485   \n",
       "4           4  1515002027079872528   \n",
       "\n",
       "                                                text    source  \n",
       "0  Scott Pruitt, Trump's former EPA chief,  is ru...  politico  \n",
       "1  RT @politicongress: A joint fundraising commit...  politico  \n",
       "2  From their earliest days, Black churches have ...  politico  \n",
       "3  RT @playbookdc: In early November, Rep. Chip R...  politico  \n",
       "4  Add West Wing Playbook to your daily reads for...  politico  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food = pd.read_csv('food_usa.csv')\n",
    "data_politics = pd.read_csv('politics_usa.csv')\n",
    "data_magazine = pd.read_csv('magazine_usa.csv')\n",
    "data_sport = pd.read_csv('sport_usa.csv')\n",
    "data_technology = pd.read_csv('technology_usa.csv')\n",
    "\n",
    "data_politics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "986c74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fitness = pd.read_csv('fitnes-health_usa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76dcec7",
   "metadata": {},
   "source": [
    "## Looking at raw-tweets \n",
    "\n",
    "Before anything else, we can print a couple of tweets from our datasets to see how they look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7185b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLITCS: Ukrainian defense minister quips sunken Russian warship \"a worthy diving site\" https://t.co/aaeB4lbPh3 https://t.co/K8suGZKC5a\n"
     ]
    }
   ],
   "source": [
    "print(\"POLITCS:\", data_politics.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "201cb235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOOD: Cheddar muffins are so easy, go with any meal and delicious. Your family will love them. #cheddar #muffins #recipe #easy #recipe #4ingredients #thesouthernladycooks #bread https://t.co/FNfBg0A7lk https://t.co/L9qDZZ3NSc\n"
     ]
    }
   ],
   "source": [
    "print(\"FOOD:\", data_food.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c7449a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAGAZINE: Amazon's Super Discount Outlet Store Just Dropped New Deals ‚Äî All Under $10  https://t.co/xfqxy8nodV\n"
     ]
    }
   ],
   "source": [
    "print(\"MAGAZINE:\", data_magazine.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53e3a121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TECHNOLOGY: Yep, startup prices are falling https://t.co/CcsPL4JiRf by @alex\n"
     ]
    }
   ],
   "source": [
    "print(\"TECHNOLOGY:\", data_technology.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a0253be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPORT: RT @SInow: It‚Äôs never too early to start thinking about the next college basketball season, right? üèÄ\n",
      "\n",
      "@CBB_Central gives his way-too-early‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "print(\"SPORT:\", data_sport.loc[random.randint(0,4500)].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359efed4",
   "metadata": {},
   "source": [
    "# Preprocess raw text for Sentiment analysis\n",
    "\n",
    "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "\n",
    "- Tokenizing the string\n",
    "- Lowercasing\n",
    "- Removing stop words and punctuation\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abcfe2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boost your hair care routine! #Scream star #JennaOrtega revealed her go-to beauty regimen for her luscious locks. Shop her favorite products now!\\nhttps://t.co/uwvRvZFI94'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = data_magazine.loc[5].text \n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f916c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\melih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05fccc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624de50d",
   "metadata": {},
   "source": [
    "# Remove hyperlinks, Twitter marks and styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5206f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boost your hair care routine! Scream star JennaOrtega revealed her go-to beauty regimen for her luscious locks. Shop her favorite products now!\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove old style retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "# remove hyperlinks\n",
    "tweet2 = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet2)\n",
    "\n",
    "# remove hashtags\n",
    "# only removing the hash # sign from the word\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "tweet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caa35c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boost your hair care routine! #Scream star #JennaOrtega revealed her go-to beauty regimen for her luscious locks. Shop her favorite products now!\\nhttps://t.co/uwvRvZFI94'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e350632",
   "metadata": {},
   "source": [
    "# Tokenize the string\n",
    "To tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af2afc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22f183da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boost your hair care routine! Scream star JennaOrtega revealed her go-to beauty regimen for her luscious locks. Shop her favorite products now!\n",
      "\n",
      "['boost', 'your', 'hair', 'care', 'routine', '!', 'scream', 'star', 'jennaortega', 'revealed', 'her', 'go-to', 'beauty', 'regimen', 'for', 'her', 'luscious', 'locks', '.', 'shop', 'her', 'favorite', 'products', 'now', '!']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "print(tweet2)\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9925a2",
   "metadata": {},
   "source": [
    "### Remove Stop Words and Punctuations\n",
    "\n",
    "The next step is to remove stop words and punctuation. Stop words are words that don't add significant meaning to the text. \n",
    "You'll see them below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cffc3bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuation\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "print('Stop words\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "519b810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed stop words and punctuation:\n",
      "['boost', 'hair', 'care', 'routine', 'scream', 'star', 'jennaortega', 'revealed', 'go-to', 'beauty', 'regimen', 'luscious', 'locks', 'shop', 'favorite', 'products']\n"
     ]
    }
   ],
   "source": [
    "tweets_clean = []\n",
    "\n",
    "for word in tweet_tokens: \n",
    "    if (word not in stopwords_english and word not in string.punctuation): \n",
    "        tweets_clean.append(word)\n",
    "\n",
    "print('removed stop words and punctuation:')\n",
    "print(tweets_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a31430",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n",
    "\n",
    "Consider the words: \n",
    " * **learn**\n",
    " * **learn**ing\n",
    " * **learn**ed\n",
    " * **learn**t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30913826",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b488357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed words:\n",
      "['boost', 'hair', 'care', 'routin', 'scream', 'star', 'jennaortega', 'reveal', 'go-to', 'beauti', 'regimen', 'lusciou', 'lock', 'shop', 'favorit', 'product']\n"
     ]
    }
   ],
   "source": [
    "tweets_stem = [] \n",
    "\n",
    "for word in tweets_clean:\n",
    "    stem_word = stemmer.stem(word)  # stemming word\n",
    "    tweets_stem.append(stem_word)  # append to the list\n",
    "\n",
    "print('stemmed words:')\n",
    "print(tweets_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ff1e5",
   "metadata": {},
   "source": [
    "# Create A Function That do pre-processing tweets one by one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "16e8f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5a678ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boost',\n",
       " 'hair',\n",
       " 'care',\n",
       " 'routin',\n",
       " 'scream',\n",
       " 'star',\n",
       " 'jennaortega',\n",
       " 'reveal',\n",
       " 'go-to',\n",
       " 'beauti',\n",
       " 'regimen',\n",
       " 'lusciou',\n",
       " 'lock',\n",
       " 'shop',\n",
       " 'favorit',\n",
       " 'product']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5dacd3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muffin', 'tin', 'great', 'meal', 'prep', 'individu', 'serv']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_tweet(data_food.loc[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4245e297",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea9d3606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1516326752834002944</td>\n",
       "      <td>A reminder that delivery app restaurants aren'...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1516318179894640641</td>\n",
       "      <td>Braised chicken legs, soy sauce, vinegar, blac...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1516311648889417728</td>\n",
       "      <td>Muffin tins are great for meal prepping indivi...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1516296543791456260</td>\n",
       "      <td>We tested everything from spicy dill pickle fl...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1516290031522127876</td>\n",
       "      <td>Is eating a regular Big Mac the secret to a lo...</td>\n",
       "      <td>foodandwine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             tweet_id  \\\n",
       "0           0  1516326752834002944   \n",
       "1           1  1516318179894640641   \n",
       "2           2  1516311648889417728   \n",
       "3           3  1516296543791456260   \n",
       "4           4  1516290031522127876   \n",
       "\n",
       "                                                text       source  \n",
       "0  A reminder that delivery app restaurants aren'...  foodandwine  \n",
       "1  Braised chicken legs, soy sauce, vinegar, blac...  foodandwine  \n",
       "2  Muffin tins are great for meal prepping indivi...  foodandwine  \n",
       "3  We tested everything from spicy dill pickle fl...  foodandwine  \n",
       "4  Is eating a regular Big Mac the secret to a lo...  foodandwine  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9233a801",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_food_processed = np.array([process_tweet(item) for item in data_food['text'].tolist()], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43f9c39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4881,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food_processed.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "801f131d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['remind', 'deliveri', 'app', 'restaur', 'alway', 'seem']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1828d340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muffin', 'tin', 'great', 'meal', 'prep', 'individu', 'serv']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food_processed[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfcf7fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['four', 'ingredi', 'one', 'bowl', 'five', 'minut', 'stir', 'togeth']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_food_processed[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba6c7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_magazine_processed = np.array([process_tweet(item) for item in data_magazine['text'].tolist()], dtype=object)\n",
    "data_technolohy_processed = np.array([process_tweet(item) for item in data_technology['text'].tolist()], dtype=object)\n",
    "data_politics_processed = np.array([process_tweet(item) for item in data_politics['text'].tolist()], dtype=object)\n",
    "data_sport_processed = np.array([process_tweet(item) for item in data_sport['text'].tolist()], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8a1ca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fitness_processed = np.array([process_tweet(item) for item in data_fitness['text'].tolist()], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dd4ba74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['internet',\n",
       " 'troll',\n",
       " 'field',\n",
       " 'day',\n",
       " \"jadensmith'\",\n",
       " 'expens',\n",
       " \"willsmith'\",\n",
       " 'son',\n",
       " 'butt',\n",
       " 'joke',\n",
       " 'say',\n",
       " 'like',\n",
       " 'talk',\n",
       " 'topic']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_magazine_processed[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6dcd79a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['earli',\n",
       " 'novemb',\n",
       " 'rep',\n",
       " 'chip',\n",
       " 'roy',\n",
       " 'text',\n",
       " 'mark',\n",
       " 'meadow',\n",
       " 'say',\n",
       " '‚Äú',\n",
       " 'need',\n",
       " 'ammo',\n",
       " '‚Äù',\n",
       " 'former',\n",
       " 'presid',\n",
       " 'donald',\n",
       " \"trump'\",\n",
       " 'effort',\n",
       " '‚Ä¶']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_politics_processed[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90b852eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'get',\n",
       " 'rockin',\n",
       " 'foam',\n",
       " 'rollin',\n",
       " 'last',\n",
       " 'time',\n",
       " 'show',\n",
       " 'way',\n",
       " 'use',\n",
       " 'foam',\n",
       " 'roller',\n",
       " 'massag',\n",
       " 'sore',\n",
       " 'muscl',\n",
       " 'use',\n",
       " 'versatil',\n",
       " 'tool',\n",
       " 'stretch',\n",
       " 'far',\n",
       " 'beyond',\n",
       " 'whether',\n",
       " 'pre',\n",
       " 'post-workout',\n",
       " 'stretch',\n",
       " 'time',\n",
       " 'feel',\n",
       " 'differ',\n",
       " 'foam',\n",
       " 'make']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fitness_processed[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9025aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
