import ast
import re
import string
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer
import math


# ----------------------------------
# Processor

def process_tweet(tweet):
    """Process tweet function.
    Input:
        tweet: a string containing a tweet
    Output:
        tweets_clean: a list of words containing the processed tweet
    """
    stemmer = PorterStemmer()
    stopwords_english = stopwords.words('english')
    tweet = re.sub(r'\$\w*', '', tweet)
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    tweet = re.sub(r'https?://[^\s\n\r]+', '', tweet)
    tweet = re.sub(r'#', '', tweet)
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                               reduce_len=True)
    tweet_tokens = tokenizer.tokenize(tweet)

    tweets_clean = []
    for word in tweet_tokens:
        if (word not in stopwords_english and
                word not in string.punctuation):
            stem_word = stemmer.stem(word)
            tweets_clean.append(stem_word)

    return tweets_clean


# ----------------------------------
# Helpers
def generate_tweets_list(dataframe_single):
    """
    Parameters
    ----------
    dataframe_single : dataframe
        It is a dataset that you collected before.

    Returns
    -------
    train_x_arr : list
        Converted version of the argument
    """
    train_x_arr = []
    for index, row in dataframe_single.iterrows():
        for a in row.values:
            train_x_arr.append(ast.literal_eval(a))

    return train_x_arr


def extract_features(tweet, freqs):
    """
    Input:
        tweet: a list of words for one tweet
        freqs: a dictionary corresponding to the frequencies of each tuple
    Output:
        x: a feature vector of dimension (1, 3)
    """
    x = np.zeros((1, 3))
    x[0, 0] = 1  # bias term

    for word in tweet:
        x[0, 1] += freqs.get((word, 1.0), 0)

        x[0, 2] += freqs.get((word, 0.0), 0)

    assert (x.shape == (1, 3))
    return x


def build_freqs(train_x, train_y):
    """
    Parameters
    ----------
    train_x : list
        list of your X
    train_y : list
        List of your Y
    Returns
    -------
    freqs : dict
        it returns dictionary that includes key and values like below;
        (word, label) : number of times appeared
    """

    yslist = np.squeeze(train_y).tolist()

    freqs = {}
    for y, tweet in zip(yslist, train_x):
        for word in tweet:
            pair = (word, y)
            if pair in freqs:
                freqs[pair] += 1
            else:
                freqs[pair] = 1

    return freqs


# --------------------------------
# Naive Bayes Functions 

def count_tweets(tweets, ys):
    """
    Input:
        tweets: a list of tweets
        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)
    Output:
        result: a dictionary mapping each pair to its frequency
    """
    result = dict()

    for y, tweet in zip(ys, tweets):
        for word in tweet:

            pair = (word, y)
            # if the key exists in the dictionary, increment the count
            if pair in result:
                result[pair] += 1
            # else, if the key is new, add it to the dictionary and set the count to 1
            else:
                result[pair] = 1

    return result


def lookup(freqs, word, label):
    """
    Parameters
    ----------
    freqs : dictionary that's generated by count_tweets funct

    word : string
       one of the words in the tweet
    label : float
        tweet's class

    Returns
    -------
    n : integer
        value that is corresponding to the key (word, label)

    """
    n = 0  # freqs.get((word, label), 0)

    pair = (word, label)
    if (pair in freqs):
        n = freqs[pair]

    return n


def naive_bayes_predict(tweet, logprior, loglikelihood):
    """
    Input:
        tweet: a processed tweet
        logprior: a number
        loglikelihood: a dictionary of words mapping to numbers
    Output:
        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)
    """
    p = 0
    # add the logprior
    p += logprior

    for word in tweet:

        # check if the word exists in the loglikelihood dictionary
        if word in loglikelihood:
            # add the log likelihood of that word to the probability
            p += loglikelihood[word]

    return p


def sigmoid(x):
    return 1 / (1 + math.exp(-x))
